# -*- coding: utf-8 -*-
"""GA_IELM_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fcE01_nR283JS9_LfCB6HfTOiKEs-9U_
"""

import numpy as np
import random
import time
import pandas as pd
import struct
import os
from sklearn import preprocessing
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

train = pd.read_csv("/content/drive/My Drive/Sample_data/mnist_train.csv")
test = pd.read_csv("/content/drive/My Drive/Sample_data/mnist_test.csv")

train = shuffle(train)
test = shuffle(test)

x_train = np.nan_to_num(train.iloc[:, 1:].values.astype('float32'), 0)
l_train = np.nan_to_num(train.iloc[:, 0].values.astype('int32'), 0) # (39575,) 

x_test = np.nan_to_num(test.iloc[:, 1:].values.astype('float32'), 0)
l_test = np.nan_to_num(test.iloc[:, 0].values.astype('int32'), 0) # (10000,)

x_max = np.max(x_train) # 255
x_max_t = np.max(x_test) # 255

# normalize data
x_train = x_train/x_max #(39575, 784)
x_test = x_test/x_max_t #(10000, 784)

fig = plt.figure(figsize=(12, 12))
for i in range(5):
    fig.add_subplot(1, 5, i+1)
    plt.title('Label: {l_train}'.format(l_train=l_train[i]))
    plt.imshow(x_train[i].reshape(28, 28), cmap='Greys')

input_number = x_train.shape[1] # 784
feature_number = x_train.shape[0] # 39575
test_input = x_test.shape[1] # 784
test_feature = x_test.shape[0] # 10000
l_max = 100

CLASSES = 10
def get_y(labels, classes):
  y = np.zeros([labels.shape[0], classes])
  for i in range(labels.shape[0]):
        y[i][labels[i]] = 1
  
  return y.T

# y_train = temp_y_train # (10, 39575)
# y_test = temp_y_test # (10, 10000)
y_train = get_y(l_train, CLASSES)
y_test = get_y(l_test, CLASSES)

def sigmoid(x):
  return 1. / (1. + np.exp(-x))

def least_mean_square(x):
  error = np.sqrt(np.square(x).mean())
  return error

# get_accuracy
def get_accuracy(pred, true):
  correct = 0
  label_numbers = true.shape[1]
  #print(label_numbers)
  for i in range(label_numbers):
    col_predicted = np.argmax(pred[:,i])
    col_true = np.argmax(true[:,i])
    #if i < 10:
    #  print("true", col_true)
    #  print("pred", col_predicted)
    if col_predicted == col_true:
      correct += 1  
    else:
      correct += 0 
  accuracy = correct/label_numbers

  return accuracy

generation = 2
pop_size = 4
pop_w = np.random.uniform(-1.,1.,size=(pop_size, input_number))
pop_b = np.random.uniform(0., 1.,size=(pop_size, 1))
pop = np.hstack([pop_w, pop_b]) #(4, 785)

time_start = time.time()

for hidden in range(l_max):
  
  if hidden == 0:
    rmse = []
    error_storage = y_train
    pop_w = np.random.uniform(-1.,1.,size=(pop_size, input_number))
    pop_b = np.random.uniform(0., 1.,size=(pop_size, 1))
    pop = np.hstack([pop_w, pop_b]) #(4, 785)
    # for optimize weights and bias
    for g in range(generation):
      fitness = []
      for p in range(pop_size):
        
        weights = pop[p, :input_number].reshape(1, input_number)# np.random.uniform(-1.,1.,size=(1, input_number))
        bias =  pop[p, -1].reshape(1,1) #random.uniform(0., 1.)
        linear = np.dot(weights, x_train.T)
        linear_bias = linear + bias
        hidden_input = sigmoid(linear_bias)
        H_inver = np.linalg.inv(np.identity(1)/CLASSES + np.dot(hidden_input, hidden_input.T)).dot(hidden_input)
        beta = np.dot(H_inver, error_storage.T)
        updated_beta = beta

        y_pred = np.dot(sigmoid(np.dot(weights, x_train.T) + bias).T, beta).T
        fit = get_accuracy(y_pred, y_train)
        fitness.append(fit)
      
      #select parents, sort by the largest accuracy
      population = []
      fit_index = sorted(range(len(fitness)), key=lambda k: fitness[k])
      fit_index = fit_index[::-1]
      print("   In generation", g+1,"the population fitness is: ", fitness)
      for i in fit_index:
        population.append(pop[i,:])

      pop = np.asarray(population)
      parent_number = int(pop_size/2)
      child_number = pop_size - parent_number
      parent = pop[:parent_number,:]
      child = np.random.uniform(-1.,1.,size=(child_number, input_number+1))

      #corss_point = random.randint(1,parent.shape[1]-1)
      #crossover
      for c in range(child_number):
        # choose random crossover point
        cross_point = random.randint(1,parent.shape[1]-1)
        #split parent into 2 groups
        mother = c%parent_number
        father = (c+1)%parent_number
        # crossover
        child[c, 0:cross_point] = parent[mother, 0:cross_point]
        child[c, cross_point:] = parent[father, cross_point:]

      # mutation
      
      for m in range(child_number):
        # choose random mutation point
        mutation_point = random.randint(1,parent.shape[1]-1)
        # set random muation value
        mutation = random.uniform(-1., 1.)
        # mutate
        child[m, mutation_point] = mutation
      
      pop = np.concatenate((parent, child))

    
    weights = pop[-1, :input_number].reshape(1, input_number)
    bias = pop[-1, -1].reshape(1,1)
    linear = np.dot(weights, x_train.T)
    linear_bias = linear + bias
    hidden_input = sigmoid(linear_bias)
    H_inver = np.linalg.inv(np.identity(1)/CLASSES + np.dot(hidden_input, hidden_input.T)).dot(hidden_input)
    beta = np.dot(H_inver, error_storage.T)
    updated_beta = beta

    error = least_mean_square(error_storage)
    rmse.append(error)
  else:
    pop_w = np.random.uniform(-1.,1.,size=(pop_size, input_number))
    pop_b = np.random.uniform(0., 1.,size=(pop_size, 1))
    pop = np.hstack([pop_w, pop_b]) #(4, 785)
    for g in range(generation):
      fitness = []
      for p in range(pop_size):        
        weight_stack = pop[p, :input_number].reshape(1, input_number)# np.random.uniform(-1.,1.,size=(1, input_number))
        bias_stack =  pop[p, -1].reshape(1,1) #random.uniform(0., 1.)
        temp_linear = np.dot(weight_stack, x_train.T)
        temp_linear_bias = temp_linear + bias_stack
        temp_hidden_input = sigmoid(temp_linear_bias)
        temp_H_inver = np.linalg.inv(np.identity(1)/CLASSES + np.dot(temp_hidden_input, temp_hidden_input.T)).dot(temp_hidden_input)
        temp_beta = np.dot(temp_H_inver, error_storage.T)
        if g ==0 and p ==0:
          weights = np.vstack([weights,weight_stack])
          bias = np.vstack([bias, bias_stack])
          updated_beta = np.vstack([updated_beta, temp_beta])
        else:
          weights[-1, :] = weight_stack
          bias[-1, :] = bias_stack
          updated_beta[-1, :] = temp_beta
        
        y_pred = np.dot(sigmoid(np.dot(weights, x_train.T) + bias).T, updated_beta).T
        fit = get_accuracy(y_pred, y_train)
        fitness.append(fit)

      #select parents, sort by the largest accuracy
      population = []
      fit_index = sorted(range(len(fitness)), key=lambda k: fitness[k])
      fit_index = fit_index[::-1]
      print("   In generation", g+1,"the population fitness is: ", fitness)
      for i in fit_index:
        population.append(pop[i,:])

      pop = np.asarray(population)
      parent_number = int(pop_size/2)
      child_number = pop_size - parent_number
      parent = pop[:parent_number,:]
      child = np.random.uniform(-1.,1.,size=(child_number, input_number+1))

      #corss_point = random.randint(1,parent.shape[1]-1)
      #crossover
      for c in range(child_number):
        cross_point = random.randint(1,parent.shape[1]-1)
        mother = c%parent_number
        father = (c+1)%parent_number
        # print("mother ", mother, "father ", father, "cross_point", cross_point)
        child[c, 0:cross_point] = parent[mother, 0:cross_point]
        child[c, cross_point:] = parent[father, cross_point:]

      # mutation
      
      for m in range(child_number):
        mutation_point = random.randint(1,parent.shape[1]-1)
        mutation = random.uniform(-1., 1.)
        child[m, mutation_point] = mutation
      
      pop = np.concatenate((parent, child))

    weight_stack = pop[-1, :input_number].reshape(1, input_number)
    weights[-1, :] = weight_stack
    bias_stack = pop[-1, -1].reshape(1,1)
    bias[-1, :] = bias_stack
    linear = np.dot(weight_stack, x_train.T)
    linear_bias = linear + bias_stack
    hidden_input = sigmoid(linear_bias)
    H_inver = np.linalg.inv(np.identity(1)/CLASSES + np.dot(hidden_input, hidden_input.T)).dot(hidden_input)
    beta = np.dot(H_inver, error_storage.T)
    updated_beta[-1, :] = beta
    error_storage = error_storage - np.dot(hidden_input.T, beta).T
    error = least_mean_square(error_storage)
    rmse.append(error)

  print("When hidden node reaches ", hidden+1, ", the error is ", error) # print error results

time_stop = time.time()

temp_train = np.dot(weights, x_train.T) + bias #(100, 10000)
temp_h_train = sigmoid(temp_train).T # (10000, 100)
y_pred_train = np.dot(temp_h_train, updated_beta).T #(10, 10000)
#y_pred_train = np.dot(sigmoid(np.dot(weights, x_train.T) + bias).T, updated_beta).T

accuracy_train = get_accuracy(y_pred_train, y_train)

print("Training accuracy:", accuracy_train)
print("training time: ", time_stop - time_start)

# predict y with test features
temp = np.dot(weights, x_test.T) + bias #(100, 10000)
temp_h = sigmoid(temp).T # (10000, 100)
y_pred = np.dot(temp_h, updated_beta).T #(10, 10000)
#y_pred_test = np.dot(sigmoid(np.dot(weights, x_test.T) + bias).T, updated_beta).T

time_start = time.time()
accuracy_test = get_accuracy(y_pred, y_test)
time_stop = time.time()
print("Testing accuracy:", accuracy_test)
print("Testing time: ", time_stop - time_start)